############################################
### Sample Data Act Broker Configuration ###
############################################


broker:

    # Specify the url where the front end of the application will be accessed.
    # For a local installation this will most likely be localhost or the 
    # location where the /public files are located. If a port is required, 
    # it should be appended to the URL.
    full_url: http://localhost:3000 


    # Set the key (string) used to serialize tokens for email validations.
    email_token_key: "1234"

    # Specify valid email address to be used as reply-to for
    # administrative emails sent by the broker.
    reply_to_email: valid.email@domain.com 

    # Set valid email address and password to be used as the DATA Act
    # broker's admin account. This is what you will use to log into
    # the broker website. The password should contain a combination
    # of letters, numbers, and special characters.
    admin_email: valid.email@domain.com
    admin_password: AdminP@ssw0rd!

    # The path where the broker will store submitted files and emails
    # when the broker is not using an AWS S3 bucket.
    # This should be an absolute path. Note that this path
    # is ignored if use_aws is true.
    broker_files: /full/path/to/broker/files

    ## Smartronix API URLs ##

    # File D1 API
    # Uses "{}" because it requires string formatting to insert query parameters
    # If the URL is provided, the file name provided will be used when uploading to S3
    award_procurement_url: https://data-usas.uat.workplace.gov/api/contracts?CGAC={}&date_signed_beginning={}&date_signed_end={}&callback={}
    award_procurement_file_name: d1_data.csv

    # File D2 API
    # If the URL is provided, the file name provided will be used when uploading to S3
    award_url: https://data-usas.uat.workplace.gov/api/assistance?CGAC={}&date_signed_beginning={}&date_signed_end={}&callback={}
    award_file_name: d2_data.csv

    d_file_storage_path: /full/path/to/d/file/storage

    # File E
    awardee_attributes_url: https://sample.gov
    awardee_attributes_file_name: awardee_data.csv

    # File F
    sub_award_url: https://sample.gov
    sub_award_file_name: sub_award_data.csv

    ## AWS Configuration Settings ##

    # If set to true the application will use AWS for the storage of files 
    # submitted to the broker, to send e-mail, AND to access the dynamo db
    # for session handling.
    # Note that you must have the aws cli installed and credentials set in  
    # order to use AWS (see install instructions for more information).
    use_aws: false 

    # If using AWS, set your region here
    aws_region: us-east-1

    # Add your AWS Key to use for sending SES emails
    aws_access_key_id: ACCESSKEYID123
    aws_secret_access_key: test123abc/123testsecretkeY

    # Name of AWS S3 bucket for uploaded broker files. Ignored if use_aws
    # is false. NOTE: the dummy value below MUST be changed to the correct
    # value if use_aws is true.
    aws_bucket: sample-aws-bucket-name

    # Include folder and filename of RSS file if running on AWS, if running locally the RSS folder and file should be in
    # the broker_files directory specified above
    rss_folder: rss
    rss_file: rss_file.xls

    # Name of the AWS role you're using to upload broker files. Ignored if
    # use_aws is false. NOTE: the dummy value below MUST be changed to the
    # correct value if use_aws is true.
    aws_role: arn:aws:iam::123456789012:role/roleName
 
    # Set the following to true to allow the broker to create temporary
    # AWS credentials for uploading files. Ignored if use_aws is false.
    aws_create_temp_credentials: true

    # S3 filenames for SF-133 file, only required if planning to load SF-133 table
    sf_133_folder: config

    # Static Files Locations
    static_files_bucket: sample-static-files-bucket
    help_files_path: sample-help-files-folder

services:
    # Set to true to turn on tracing rest errors.
    rest_trace: true

    # Set to true to turn on server debugging.
    server_debug: true

    # URL/IP address that hosts the broker API
    broker_api_host: 127.0.0.1
    broker_api_port: 3333

    # URL/IP address that hosts the validator API
    validator_host: 127.0.0.1
    validator_port: 3334

    # If you would like to restrict access from other origins, set the
    # allowed origins here. Otherwise, leave as '*'.
    cross_origin_url: '*'

    # The path where the broker will store error reports
    # generated by the validator. If you're running
    # everything on the same server, consider using the
    # same path as the broker_files setting (above)
    # for simplicity.
    # This should be an absolute path. Note that this path
    # is ignored if use_aws is true.
    error_report_path: /full/path/to/error/reports

    # We import award/subaward records from FSRS
    fsrs:
        procurement_service:
            wsdl: ''  # e.g. https://example.com/?wsdl'
            username: ''
            password: ''
        grant_service:
            wsdl: ''  # e.g. https://example.com/?wsdl'
            username: ''
            password: ''

db:
    # The name of your default postgres database. Unless you've exlicitly
    # changed this, you should not have to update the value below.
    base_db_name: postgres

    # The type of backend database being used
    scheme: postgres

    # Host and port of db instance. Set to localhost if running locally or set 
    # the remote address
    host: localhost
    port: 5432

    # Set your username and password for the db instance
    username: yourusername
    password: yourpassword

    # Set the names for each of the application databases
    base_db_name: postgres #This is the default db on the instance. 
    db_name: data_broker
    job_queue_db_name: da_job_queue # Job queue db

    # The broker uses DynamoDb for session management. If
    # use_aws is true above, the broker will use a
    # Dynamo instance on your AWS account. Otherwise,
    # provide a dynamo host and port below.
    dynamo_host: 127.0.0.1
    dynamo_port: 8000

logging:

    # The path where broker still store log files.
    # Ignored if use_logstash is true.
    log_files: /full/path/to/logfiles

    # Set following to true and fill in all logstash_ settings
    # to use logstash for logging
    use_logstash: false

    # Logstash host. Ignored if use_logstash is false.
    # NOTE: the dummy value below MUST be changed to the correct
    # value if use_logstash is true.
    logstash_host: logstash-host.compute-1.amazonaws.com

    # Logstash port. 514 is the port typically used with logstash,
    # so you can mostly likely keep this as is.
    logstash_port: 514

job-queue:
    # RabbitMQ username
    username: guest

    # RabbitMQ password
    password: guest

    # URL for the job queue server
    url: 127.0.0.1

    # Port on which Celery is listening
    port: 5672

    # Broker scheme which must coordinate with the type of broker being used
    # on the remote server. amqp corresponds to RabbitMQ.
    broker_scheme: amqp
