############################################
### Sample Data Act Broker Configuration ###
############################################


broker:

    # Specify the number or rows to process at a time during submission validation
    validator_batch_size: 10000

    # Set to true to run multiple processes concurrently when loading in data
    parallel_loading: false
    # Specify the number of multiprocessing pools (0 uses python's os.cpu_count())
    multiprocessing_pools: 0

    # batch the results of SQL Validations (to save on memory)
    batch_sql_validation_results: true

    # Specify the url where the front end of the application will be accessed.
    # For a local installation this will most likely be localhost or the
    # location where the /public files are located. If a port is required,
    # it should be appended to the URL.
    full_url: http://localhost:3000

    sqs_queue_name: sqs-queue-name


    # Set the key (string) used to serialize tokens for email validations.
    email_token_key: "token123"

    # Specify valid email address to be used as reply-to for
    # administrative emails sent by the broker.
    reply_to_email: valid.email@domain.com

    # Set valid email address and password to be used as the DATA Act
    # broker's admin account. This is what you will use to log into
    # the broker website. The password should contain a combination
    # of letters, numbers, and special characters.
    admin_email: valid.email@domain.com
    admin_password: password123

    # The path where the broker will store submitted files and emails
    # when the broker is not using an AWS S3 bucket.
    # This should be an absolute path. Note that this path
    # is ignored if use_aws is true.
    broker_files: /data-act/backend/tmp

    # The folder within dataactvalidator/config where USPS zip4 files are stored
    zip_folder: zips

    d_file_storage_path: /data-act/backend/tmp/

    # File E
    awardee_attributes_file_name: awardee_data.csv

    ## AWS Configuration Settings ##

    # If set to true the application will use AWS for the storage of files
    # submitted to the broker, to send e-mail, AND to access the dynamo db
    # for session handling.
    # Note that you must have the aws cli installed and credentials set in
    # order to use AWS (see install instructions for more information).
    use_aws: false

    # If using AWS, set your region here
    aws_region: us-gov-region-1

    # Add your AWS Key to use for sending SES emails
    aws_access_key_id: redacted
    aws_secret_access_key: redacted

    # Name of AWS S3 bucket for uploaded broker files. Ignored if use_aws
    # is false. NOTE: the dummy value below MUST be changed to the correct
    # value if use_aws is true.
    aws_bucket: s3-bucket

    # Region for pulling CFDA file, only required if planning to load CFDA file
    cfda_region: us-east-1

    # Bucket for pulling CFDA file, only required if planning to load CFDA file
    cfda_bucket: sample-cfda-bucket-name

    # Only required if planning to load CFDA file	+    sf_133_bucket: s3-bucket
    cfda_file_path: sample-cfda-file-path

    sf_133_bucket: s3-bucket
    archive_bucket: s3-bucket

    # Name of AWS S3 bucket for certified broker files. Ignored if use_aws
    # is false. NOTE: the dummy value below MUST be changed to the correct
    # value if use_aws is true.
    certified_bucket: s3-bucket

    # Link to replace the amazon portion of the url
    proxy_url: new-url.com

    submission_bucket_mapping: [s3-bucket, proxy/here]
    certified_bucket_mapping: [s3-bucket, proxy/here]

    # S3 filenames for SF-133 file, only required if planning to load SF-133 table
    sf_133_folder: config
    sf_133_file: sf_133.csv

    # Location that deleted records from the FPDS atom feed are stored
    fpds_delete_bucket: s3-bucket

    # link to help for failed MAX login
    max_help_url: https://max_help_url.gov

    # MAX login
    cas_service_url: https://cas.service.url
    parent_group: sample

    max:
        federal_hierarchy_api_url: https://federal.hierarchy.api.uri.gov/?api_key={}
        federal_hierarchy_api_key: example

    # Session timeout, in seconds
    session_timeout: 1800

services:
    debug: true

    # URL/IP address that hosts the broker API
    broker_api_host: 0.0.0.0
    broker_api_port: 9999

    # URL/IP address that hosts the validator API
    validator_host: 0.0.0.0
    validator_port: 8889

    # If you would like to restrict access from other origins, set the
    # allowed origins here. Otherwise, leave as '*'.
    cross_origin_url: '*'

    # The path where the broker will store error reports
    # generated by the validator. If you're running
    # everything on the same server, consider using the
    # same path as the broker_files setting (above)
    # for simplicity.
    # This should be an absolute path.
    error_report_path: /data-act/backend/tmp

    # The paths to the sample D1 and D2 files for local development
    d2_file_path: /data-act/backend/tests/integration/data/d2_sample.csv
    d1_file_path: /data-act/backend/tests/integration/data/d1_sample.csv

    # We import award/subaward records from FSRS
    fsrs:
       procurement_service:
           wsdl: ''  # e.g. https://example.com/?wsdl'
           username: ''
           password: ''
       grant_service:
           wsdl: ''  # e.g. https://example.com/?wsdl'
           username: ''
           password: ''


db:
    # The name of your default postgres database. Unless you've exlicitly
    # changed this, you should not have to update the value below.
    base_db_name: postgres

    # The scheme (type of database) being used. Ex: postgres, mysql, etc.
    scheme: postgres

    # Host and port of db instance. Set to localhost if running locally or set
    # the remote address
    # host: broker-sbx.cvhpn1mlakcs.us-gov-west-1.rds.amazonaws.com
    host: dataact-broker-db
    port: 5432

    # Set your username and password for the db instance
    username: admin
    password: root

    # Set the names for each of the application databases
    base_db_name: postgres #This is the default db on the instance.
    user_db_name: user_manager # User manager db
    job_db_name: job_tracker # Job tracker db
    staging_db_name: staging # Staging db
    validator_db_name: validation # Validation db
    error_db_name: error_data # Error data db
    job_queue_db_name: job_queue # Job queue db
    db_name: data_broker

    # The broker uses DynamoDb for session management. If
    # use_aws is true above, the broker will use a
    # Dynamo instance on your AWS account. Otherwise,
    # provide a dynamo host and port below.
    dynamo_host: 127.0.0.1
    dynamo_port: 5555

logging:

    # The path where broker still store log files.
    # Ignored if use_logstash is true.
    log_files: /data-act/backend/tmp/logs
    # Set to true and fill in all logstash_ settings
    # to use logstash for logging
    use_logstash: false

    # Logstash EC2 host. Ignored if use_logstash is false.
    # NOTE: the dummy value below MUST be changed to the correct
    # value if use_logstash is true.
    logstash_host: logstash-host.compute-1.amazonaws.com

    # Logstash EC2 port. 514 is the port typically used with logstash,
    # so you can mostly likely keep this as is.
    logstash_port: port
    
